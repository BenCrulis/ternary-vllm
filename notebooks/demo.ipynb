{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse as ap\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import lite\n",
    "from tensorflow.lite.python.interpreter import InterpreterWithCustomOps\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from larq_compute_engine.tflite.python.interpreter import Interpreter\n",
    "from larq_compute_engine.tflite.python.interpreter_wrapper_lite import register_tflite_all_ops\n",
    "\n",
    "\n",
    "from training.datasets.llava import LLavaDS\n",
    "from utils.scripting import get_var\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from ../checkpoints/moondream-q2-1-1-001\n",
      "using 4 thread(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 10:58:45.802273: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_inter_op_parallelism which is not in the op definition: Op<name=StridedSlice; signature=input:T, begin:Index, end:Index, strides:Index -> output:T; attr=T:type; attr=Index:type,allowed=[DT_INT16, DT_INT32, DT_INT64]; attr=begin_mask:int,default=0; attr=end_mask:int,default=0; attr=ellipsis_mask:int,default=0; attr=new_axis_mask:int,default=0; attr=shrink_axis_mask:int,default=0> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node StridedSlice}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished encoding image\n",
      "end of sequence is token 50256\n",
      "prompt is encoded\n",
      "<image>\n",
      "\n",
      "Question: Describe this image.\n",
      "\n",
      "Answer: The\n",
      "image decoding done in 70.18\n",
      "<image>\n",
      "\n",
      "Question: Describe this image.\n",
      "\n",
      "Answer: image shows a red stop sign with graffiti on it, placed on a street corner.\n",
      "cache shape (24, 2, 1, 32, 761, 64)\n",
      "done in 12.50s (7.92t/s)\n",
      "(1, 11, 51200)\n",
      "(24, 2, 1, 32, 11, 64)\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    file=\"../moondream-q2-matmul.tflite\",\n",
    "    threads=4,\n",
    "    model=\"vikhyatk/moondream2\",\n",
    "    revision=\"2024-07-23\"\n",
    ")\n",
    "\n",
    "# MODEL = args.model\n",
    "MODEL = Path(\"../checkpoints/moondream-q2-1-1-001\")\n",
    "\n",
    "MD_REVISION = args.revision\n",
    "\n",
    "\n",
    "DTYPE=torch.float32\n",
    "DEVICE=\"cpu\"\n",
    "\n",
    "ANSWER_EOS = \"<|endoftext|>\"\n",
    "\n",
    "# Number of tokens used to represent each image.\n",
    "IMG_TOKENS = 729\n",
    "\n",
    "\n",
    "print(f\"loading model from {MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\n",
    "moondream: nn.Module = AutoModelForCausalLM.from_pretrained(\n",
    "    \"vikhyatk/moondream2\", revision=MD_REVISION, trust_remote_code=True,\n",
    "    attn_implementation=None,\n",
    "    torch_dtype=DTYPE, device_map={\"\": DEVICE}\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"using {args.threads} thread(s)\")\n",
    "\n",
    "\n",
    "\n",
    "img_path = \"/home/crulis/datasets/coco/images/test2017/000000000771.jpg\"\n",
    "\n",
    "\n",
    "img = PIL.Image.open(img_path)\n",
    "\n",
    "\n",
    "# interpreter_object = Interpreter # LCE interpreter\n",
    "# interpreter_object = lite.Interpreter # TF interpreter\n",
    "interpreter_object = lambda model_path: InterpreterWithCustomOps([register_tflite_all_ops], model_path=model_path, num_threads=args.threads)\n",
    "\n",
    "# tflite_model_path = \"moondream-q2-tf.tflite\"\n",
    "# tflite_model_path = \"moondream-q2-larq.tflite\"\n",
    "tflite_model_path = str(args.file)\n",
    "\n",
    "\n",
    "if interpreter_object != Interpreter:\n",
    "    interpreter = interpreter_object(tflite_model_path)\n",
    "else:\n",
    "    with open(tflite_model_path, mode=\"rb\") as model_file:\n",
    "        interpreter = interpreter_object(model_file.read())\n",
    "# interpreter = lite.Interpreter(\"moondream-tf.tflite\")\n",
    "# predict_fn = interpreter.get_signature_runner()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # use original model to encode the image\n",
    "    enc_image = moondream.encode_image(img)\n",
    "    print(\"finished encoding image\")\n",
    "\n",
    "    if False:\n",
    "        # how we would do inference with the original model\n",
    "        sample_predicted_text = moondream.answer_question(enc_image, \"Describe this image.\", tokenizer, do_sample=False)\n",
    "        print(\"initial prediction:\", sample_predicted_text)\n",
    "    enc_image = enc_image.numpy()\n",
    "\n",
    "del moondream.text_model\n",
    "\n",
    "print(f\"end of sequence is token {tokenizer.unk_token_id}\")\n",
    "\n",
    "\n",
    "chat_history = \"\"\n",
    "question = \"Describe this image.\"\n",
    "\n",
    "prompt = f\"<image>\\n\\n{chat_history}Question: {question}\\n\\nAnswer:\"\n",
    "\n",
    "\n",
    "def input_embeds(prompt, image_embeds, text_emb, tokenizer):\n",
    "    def _tokenize(txt):\n",
    "        return tokenizer(\n",
    "            txt, return_tensors=\"np\", add_special_tokens=False\n",
    "        )[\"input_ids\"].astype(np.int64)\n",
    "\n",
    "    # Add BOS token\n",
    "    embeds = []\n",
    "    embeds.append(\n",
    "        text_emb((np.array([[tokenizer.bos_token_id]], dtype=np.int64)))\n",
    "    )\n",
    "\n",
    "    if \"<image>\" not in prompt:\n",
    "        embeds.append(text_emb(_tokenize(prompt)))\n",
    "    else:\n",
    "        assert prompt.count(\"<image>\") == 1\n",
    "        before, after = prompt.split(\"<image>\")\n",
    "        if len(before) > 0:\n",
    "            embeds.append(text_emb(_tokenize(before)))\n",
    "        embeds.append(image_embeds)\n",
    "        if len(after) > 0:\n",
    "            embeds.append(text_emb(_tokenize(after)))\n",
    "\n",
    "    return np.concatenate(embeds, axis=1)\n",
    "\n",
    "# call = interpreter.get_signature_runner(\"call\")\n",
    "\n",
    "init_cache_shape = interpreter.get_signature_runner(\"empty_cache\").get_output_details()[\"output_0\"][\"shape\"]\n",
    "\n",
    "# cache = interpreter.get_signature_runner(\"empty_cache\")()[\"output_0\"] # throw an invalid tensor size error in newer versions of tensorflow\n",
    "cache = np.zeros(init_cache_shape, dtype=np.float32)\n",
    "\n",
    "# interpreter.resize_tensor_input(0, (1, 11, 2048))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare your input data\n",
    "token_ids = np.array(np.random.randint(0, 50000, size=(1, 11)), dtype=np.int64)\n",
    "\n",
    "def embed(token_ids):\n",
    "    compute_embeddings = interpreter.get_signature_runner(\"compute_embeddings\")\n",
    "    input_data = compute_embeddings(token_ids=token_ids)[\"output_0\"]\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def predict(token_ids, cache, embeddings=None):\n",
    "    if embeddings is not None and token_ids is not None:\n",
    "        raise ValueError(\"both token_ids and embeddings cannot be specified\")\n",
    "    elif embeddings is None and token_ids is None:\n",
    "        raise ValueError(\"one of token_ids or embeddings has to be specified\")\n",
    "\n",
    "    if embeddings is None:\n",
    "        compute_embeddings = interpreter.get_signature_runner(\"compute_embeddings\")\n",
    "        input_data = compute_embeddings(token_ids=token_ids)[\"output_0\"]\n",
    "        del compute_embeddings\n",
    "    else:\n",
    "        input_data = embeddings\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "\n",
    "    inp_token_ids_idx = input_details[0][\"index\"]\n",
    "    inp_cache_idx = input_details[1][\"index\"]\n",
    "\n",
    "    # Set the input tensor\n",
    "    interpreter.resize_tensor_input(inp_token_ids_idx, input_data.shape)\n",
    "    interpreter.resize_tensor_input(inp_cache_idx, cache.shape)\n",
    "    interpreter.allocate_tensors()\n",
    "    interpreter.set_tensor(0, input_data)\n",
    "    interpreter.set_tensor(1, cache)\n",
    "\n",
    "    time_bef = time.time()\n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    elapsed = time.time() - time_bef\n",
    "    # print(f\"done in {elapsed:.3f}s\")\n",
    "\n",
    "    output_details = interpreter.get_output_details()\n",
    "    logits_idx = output_details[1][\"index\"]\n",
    "    cache_idx = output_details[0][\"index\"]\n",
    "\n",
    "    # Get the output tensor\n",
    "    logits = interpreter.get_tensor(logits_idx)\n",
    "    new_cache = interpreter.get_tensor(cache_idx)\n",
    "    return logits, new_cache\n",
    "\n",
    "\n",
    "def inference_loop(prompt, image_embed, cache, tokenizer):\n",
    "    embs = input_embeds(prompt, image_embed, embed, tokenizer)\n",
    "    print(\"prompt is encoded\")\n",
    "    print(prompt, end=\"\", flush=True)\n",
    "    pred_tokens = []\n",
    "    SEQ_LEN = 100\n",
    "    time_bef = time.time()\n",
    "    for i in range(SEQ_LEN):\n",
    "        if i == 1:\n",
    "            elapsed = time.time() - time_bef\n",
    "            print(f\"\\nimage decoding done in {elapsed:.2f}\")\n",
    "            print(prompt, end=\"\", flush=True)\n",
    "        if i <= 1:\n",
    "            time_bef = time.time()\n",
    "        logits, cache = predict(None, embeddings=embs, cache=cache)\n",
    "        next_token = logits[0, -1, :].argmax()\n",
    "        if next_token == tokenizer.unk_token_id:\n",
    "            break\n",
    "        print(tokenizer.decode(next_token), end=\"\", flush=True)\n",
    "        embs = embed(np.array([[next_token]], dtype=np.int64))\n",
    "        pass\n",
    "    print()\n",
    "    elapsed = time.time() - time_bef\n",
    "    t_per_s = (SEQ_LEN - 1) / elapsed\n",
    "    print(f\"cache shape {cache.shape}\")\n",
    "    print(f\"done in {elapsed:.2f}s ({t_per_s:.2f}t/s)\")\n",
    "    print(\"response: \", tokenizer.decode(pred_tokens))\n",
    "    pass\n",
    "\n",
    "inference_loop(prompt, enc_image, cache, tokenizer)\n",
    "\n",
    "\n",
    "logits, cache = predict(token_ids, cache=cache)\n",
    "\n",
    "print(logits.shape)\n",
    "print(cache.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
